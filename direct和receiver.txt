0.8
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-8_2.11</artifactId>
    <version>2.0.0</version>
    <scope>provided</scope>
</dependency>

KafkaUtils.createStream(ssc,zkQuorum,groupId,topics,storageLevel)：ReceiverInputDStream（调用下面方法）;

KafkaUtils.createStream[K: ClassTag, V: ClassTag, U <: Decoder[_]: ClassTag, T <: Decoder[_]: ClassTag]
								(ssc,kafkaParams,topics,storageLevel)：ReceiverInputDStream
		内 val walEnabled = WriteAheadLogUtils.enableReceiverLog(ssc.conf) //开启WAL
		new KafkaInputDStream[K, V, U, T](ssc, kafkaParams, topics, walEnabled, storageLevel)

KafkaUtils.createDirectStream[K: ClassTag,V: ClassTag,KD <: Decoder[K]: ClassTag, VD <: Decoder[V]: ClassTag,R: ClassTag]
(ssc,kafkaParam,fromOffsets: Map[TopicAndPartition, Long],messageHandler: MessageAndMetadata[K, V] => R): InputDStream[R]
		内 val cleanedHandler = ssc.sc.clean(messageHandler)
		new DirectKafkaInputDStream[K, V, KD, VD, R](ssc, kafkaParams, fromOffsets, cleanedHandler)}
		//使用该方式时需要手动new KafkaCluster(params)

KafkaUtils.createDirectStream[K: ClassTag,V: ClassTag,KD <: Decoder[K]: ClassTag,VD <: Decoder[V]: ClassTag]
(ssc: StreamingContext,kafkaParams: Map[String, String],topics: Set[String]): InputDStream[(K, V)]
		内 val messageHandler = (mmd: MessageAndMetadata[K, V]) => (mmd.key, mmd.message)
		val kc = new KafkaCluster(kafkaParams)
		val fromOffsets = getFromOffsets(kc, kafkaParams, topics)
		new DirectKafkaInputDStream[K, V, KD, VD, (K, V)](ssc, kafkaParams, fromOffsets, messageHandler)
		
		
010
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>
    <version>2.4.0</version>
</dependency>
KafkaUtils.createDirectStream[K, V](ssc,locationStrategy: LocationStrategy,consumerStrategy: ConsumerStrategy[K, V]): InputDStream[ConsumerRecord[K, V]]
    内 val ppc = new DefaultPerPartitionConfig(ssc.sparkContext.getConf)
    createDirectStream[K, V](ssc, locationStrategy, consumerStrategy, ppc)（调用下面方法）

KafkaUtils.createDirectStream[K, V](ssc: StreamingContext,locationStrategy: LocationStrategy,consumerStrategy: ConsumerStrategy[K, V],
      perPartitionConfig: PerPartitionConfig): InputDStream[ConsumerRecord[K, V]]
	内 new DirectKafkaInputDStream[K, V](ssc, locationStrategy, consumerStrategy, perPartitionConfig)
 

direct和receiver比较参考文档：https://blog.csdn.net/wzqllwy/article/details/78869889

<scope>
1.compile：默认值 他表示被依赖项目需要参与当前项目的编译，还有后续的测试，运行周期也参与其中，是一个比较强的依赖。打包的时候通常需要包含进去
2.test：依赖项目仅仅参与测试相关的工作，包括测试代码的编译和执行，不会被打包，例如：junit
3.runtime：表示被依赖项目无需参与项目的编译，不过后期的测试和运行周期需要其参与。与compile相比，跳过了编译而已。例如JDBC驱动，适用运行和测试阶段
4.provided：打包的时候可以不用包进去，别的设施会提供。事实上该依赖理论上可以参与编译，测试，运行等周期。相当于compile，但是打包阶段做了exclude操作
5.system：从参与度来说，和provided相同，不过被依赖项不会从maven仓库下载，而是从本地文件系统拿。需要添加systemPath的属性来定义路径
